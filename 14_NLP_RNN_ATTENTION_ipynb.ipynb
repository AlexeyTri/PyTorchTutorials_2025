{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpP2xw5UTq45bXHR3Aq5zt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyTri/PyTorchTutorials_2025/blob/main/14_NLP_RNN_ATTENTION_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mIlucwRXp2NS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78e5453-d91b-41b5-e3b5-168a4eeaca95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 983.2/983.2 kB 11.0 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -q torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "wV8kboswAWo-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "if torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'"
      ],
      "metadata": {
        "id": "mMRiwLbiAWyL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_tm(model, dataloader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        pred = model(X_batch)\n",
        "        metric.update(pred, y_batch)\n",
        "    return metric.compute()"
      ],
      "metadata": {
        "id": "wm0mGQdMAW2j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader, n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max',factor=factor, patience=patience)\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        metric.reset()\n",
        "        total_loss = 0.\n",
        "        if epoch_callback is not None:\n",
        "            epoch_callback(model, epoch)\n",
        "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "            train_metric = metric.compute().item()\n",
        "            print(f\"\\rBatch {index + 1}/ {len(train_loader)}\", end=\"\")\n",
        "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
        "            print(f\", {train_metric=:.2%}\", end=\"\")\n",
        "        history[\"train_losses\"].append(total_loss/len(train_loader))\n",
        "        history[\"train_metrics\"].append(train_metric)\n",
        "        val_metric = evaluate_tm(model, valid_loader, metric=metric)\n",
        "        history[\"valid_metrics\"].append(val_metric.item())\n",
        "        scheduler.step(val_metric)\n",
        "        print(f\"\\rEpoch {epoch+1}/{n_epochs},\"\n",
        "              f\"train loss: {history[\"train_losses\"][-1]:.4f},\"\n",
        "              f\"train metrics: {history[\"train_metrics\"][-1]:.2%}, \"\n",
        "              f\"valid metrics: {history[\"valid_metrics\"][-1]:.2%} \")\n",
        "    return history"
      ],
      "metadata": {
        "id": "qV598ZTTAW7N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "def del_vars(variable_names=[]):\n",
        "    for name in variable_names:\n",
        "        try:\n",
        "            del globals()[name]\n",
        "        except KeyError:\n",
        "            pass  # ignore variables that have already been deleted\n",
        "    gc.collect()\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3u_UDUhTAXAE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ],
      "metadata": {
        "id": "xApGaTzUIJ4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Training Dataset**"
      ],
      "metadata": {
        "id": "5PyJDMwiILP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "def dowload_shkspeare_text():\n",
        "    path_file = Path(\"datasets/shakespeare/shakespeare.txt\")\n",
        "    if not path_file.is_file():\n",
        "        path_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://homl.info/shakespeare\"\n",
        "        urllib.request.urlretrieve(url, path_file)\n",
        "    return path_file.read_text()"
      ],
      "metadata": {
        "id": "LfNR4YR_AXFC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespear_text = dowload_shkspeare_text()"
      ],
      "metadata": {
        "id": "Sigoc-9mAXJc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespear_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLM8n4b-AXNy",
        "outputId": "714753af-3ca3-4d00-a1cf-61a09c5c6424"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(shakespear_text.lower()))"
      ],
      "metadata": {
        "id": "A3A37H6WAXR4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HmVv8OWLAXWF",
        "outputId": "689893b0-adf4-469e-9144-74cef2cba756"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx = {char: index for index, char in enumerate(vocab)}\n",
        "index_to_char = {index: char for index, char in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "k4TjXEbYAXZz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx[\"a\"], index_to_char[13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8-p26TNAXef",
        "outputId": "3a341e5e-51d4-4c0b-8624-d51dd5f3eb80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 'a')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text):\n",
        "    return torch.tensor([char_to_idx[char] for char in text.lower()])\n",
        "def decode_text(char_ids):\n",
        "    return \"\".join([index_to_char[char_id.item()] for char_id in char_ids])"
      ],
      "metadata": {
        "id": "8GXF75fGAXis"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = encode_text(\"Hello world\")\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbI_mO1YAXnD",
        "outputId": "750831ee-ae56-4b86-84f2-856c43f7e227"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([20, 17, 24, 24, 27,  1, 35, 27, 30, 24, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_text(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4mFB7wHOAXrc",
        "outputId": "09597bf6-da32-416c-f65f-0f5b59dc8d0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, text, window_length):\n",
        "        self.encode_text = encode_text(text)\n",
        "        self.window_length = window_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encode_text) - self.window_length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.encode_text):\n",
        "            raise IndexError(\"length over len array\")\n",
        "        end = idx + self.window_length\n",
        "        window = self.encode_text[idx : end]\n",
        "        target = self.encode_text[idx+1 : end+1]\n",
        "        return window, target\n"
      ],
      "metadata": {
        "id": "_aZZK0uROOQJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_be_dataset = CharDataset(\"To be or not to be\", window_length=7)\n",
        "for x, y in to_be_dataset:\n",
        "    print(f\"x: {x}, y: {y}\")\n",
        "    print(f\"decode x: {decode_text(x)}, decode y: {decode_text(y)}\")\n"
      ],
      "metadata": {
        "id": "_KWJvixMOOUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf74c976-dc01-4607-820f-ec0f119b5f2b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([32, 27,  1, 14, 17,  1, 27]), y: tensor([27,  1, 14, 17,  1, 27, 30])\n",
            "decode x: to be o, decode y: o be or\n",
            "x: tensor([27,  1, 14, 17,  1, 27, 30]), y: tensor([ 1, 14, 17,  1, 27, 30,  1])\n",
            "decode x: o be or, decode y:  be or \n",
            "x: tensor([ 1, 14, 17,  1, 27, 30,  1]), y: tensor([14, 17,  1, 27, 30,  1, 26])\n",
            "decode x:  be or , decode y: be or n\n",
            "x: tensor([14, 17,  1, 27, 30,  1, 26]), y: tensor([17,  1, 27, 30,  1, 26, 27])\n",
            "decode x: be or n, decode y: e or no\n",
            "x: tensor([17,  1, 27, 30,  1, 26, 27]), y: tensor([ 1, 27, 30,  1, 26, 27, 32])\n",
            "decode x: e or no, decode y:  or not\n",
            "x: tensor([ 1, 27, 30,  1, 26, 27, 32]), y: tensor([27, 30,  1, 26, 27, 32,  1])\n",
            "decode x:  or not, decode y: or not \n",
            "x: tensor([27, 30,  1, 26, 27, 32,  1]), y: tensor([30,  1, 26, 27, 32,  1, 32])\n",
            "decode x: or not , decode y: r not t\n",
            "x: tensor([30,  1, 26, 27, 32,  1, 32]), y: tensor([ 1, 26, 27, 32,  1, 32, 27])\n",
            "decode x: r not t, decode y:  not to\n",
            "x: tensor([ 1, 26, 27, 32,  1, 32, 27]), y: tensor([26, 27, 32,  1, 32, 27,  1])\n",
            "decode x:  not to, decode y: not to \n",
            "x: tensor([26, 27, 32,  1, 32, 27,  1]), y: tensor([27, 32,  1, 32, 27,  1, 14])\n",
            "decode x: not to , decode y: ot to b\n",
            "x: tensor([27, 32,  1, 32, 27,  1, 14]), y: tensor([32,  1, 32, 27,  1, 14, 17])\n",
            "decode x: ot to b, decode y: t to be\n",
            "x: tensor([32,  1, 32, 27,  1, 14, 17]), y: tensor([ 1, 32, 27,  1, 14, 17])\n",
            "decode x: t to be, decode y:  to be\n",
            "x: tensor([ 1, 32, 27,  1, 14, 17]), y: tensor([32, 27,  1, 14, 17])\n",
            "decode x:  to be, decode y: to be\n",
            "x: tensor([32, 27,  1, 14, 17]), y: tensor([27,  1, 14, 17])\n",
            "decode x: to be, decode y: o be\n",
            "x: tensor([27,  1, 14, 17]), y: tensor([ 1, 14, 17])\n",
            "decode x: o be, decode y:  be\n",
            "x: tensor([ 1, 14, 17]), y: tensor([14, 17])\n",
            "decode x:  be, decode y: be\n",
            "x: tensor([14, 17]), y: tensor([17])\n",
            "decode x: be, decode y: e\n",
            "x: tensor([17]), y: tensor([], dtype=torch.int64)\n",
            "decode x: e, decode y: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_length = 50\n",
        "batch_size = 512\n",
        "\n",
        "train_size = CharDataset(shakespear_text[: 1_000_000], window_length=window_length)\n",
        "valid_size = CharDataset(shakespear_text[1_000_000 : 1_060_000], window_length=window_length)\n",
        "test_size = CharDataset(shakespear_text[1_060_000 : ], window_length=window_length)\n",
        "\n",
        "train_loader = DataLoader(train_size, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_size, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_size, batch_size=batch_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2eTEWI7aOOXt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**\n",
        "\n",
        "$emb = sqrt(n)$"
      ],
      "metadata": {
        "id": "JTdbqkizhhXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "embed = nn.Embedding(5, 3)\n",
        "embed(torch.tensor([[0, 1], [2, 1], [3, 4]]))"
      ],
      "metadata": {
        "id": "qcL23n8qOOa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a515fc2-ccee-4fe3-84c6-30447719ee3f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3367,  0.1288,  0.2345],\n",
              "         [ 0.2303, -1.1229, -0.1863]],\n",
              "\n",
              "        [[ 2.2082, -0.6380,  0.4617],\n",
              "         [ 0.2303, -1.1229, -0.1863]],\n",
              "\n",
              "        [[ 0.2674,  0.5349,  0.8094],\n",
              "         [ 1.1103, -1.6898, -0.9890]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line = nn.Linear(5, 3, bias=False)\n",
        "line.forward(torch.tensor([0., 0., 0., 1., 0.]))"
      ],
      "metadata": {
        "id": "U8Je_WfrOOlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4def3c6-a8af-4374-bf6f-20059908e042"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2764,  0.1202, -0.1955], grad_fn=<SqueezeBackward4>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line.weight.T.std(dim=1)"
      ],
      "metadata": {
        "id": "YWwIzCrMOOoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdb6c8b-20d2-4d35-d885-cfb19150b631"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0956, 0.1953, 0.2319, 0.2404, 0.1897], grad_fn=<StdBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed.weight.std(dim=1)"
      ],
      "metadata": {
        "id": "Et0boP5eOOrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472a53fc-61d0-4c4e-e423-42955f2e6d93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1039, 0.6930, 1.4353, 0.2710, 1.4571], grad_fn=<StdBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building and Training the Char-RNN Model**"
      ],
      "metadata": {
        "id": "reJIfnHgCmUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespearModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embeddings = self.embed(X)\n",
        "        outputs, _states = self.gru(embeddings)\n",
        "        return self.linear(outputs).permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "I2hQFHI9OOuP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model = ShakespearModel(len(vocab)).to(device)"
      ],
      "metadata": {
        "id": "sEg9dKMQOOw7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "xentropy = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.NAdam(model.parameters())\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(vocab)).to(device)\n",
        "\n",
        "history = train(model=model, optimizer=optimizer, loss_fn=xentropy, metric=accuracy, train_loader=train_loader, valid_loader=valid_loader, n_epochs=n_epochs)"
      ],
      "metadata": {
        "id": "7RnFi2YKOOzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f446c064-e639-45f1-88a7-19bd7921a1ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5,train loss: 1.5975,train metrics: 51.42%, valid metrics: 51.81% \n",
            "Epoch 2/5,train loss: 1.3933,train metrics: 56.47%, valid metrics: 52.01% \n",
            "Epoch 3/5,train loss: 1.3648,train metrics: 57.19%, valid metrics: 53.26% \n",
            "Epoch 4/5,train loss: 1.3511,train metrics: 57.53%, valid metrics: 52.94% \n",
            "Epoch 5/5,train loss: 1.3428,train metrics: 57.74%, valid metrics: 53.94% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"my_shakespeare_model.pt\")"
      ],
      "metadata": {
        "id": "ztsc55KHOO2W"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/my_shakespeare_model.pt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0qpWktiUAgQ",
        "outputId": "f441977f-0de4-43e9-de12-75e9cd2b420a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "text = \"To be or not to b\"\n",
        "encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
        "with torch.no_grad():\n",
        "    Y_logits = model(encoded_text)\n",
        "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
        "    predict_char = index_to_char[predicted_char_id]\n",
        "\n",
        "predict_char"
      ],
      "metadata": {
        "id": "mGcHmvkrOO5G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "13b2531a-f7b2-41f9-f8b1-fb0a02489ef3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Shakespearean Text**"
      ],
      "metadata": {
        "id": "IqqmyAataV0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def next_char(model, text, temperature=1):\n",
        "    encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
        "    with torch.no_grad():\n",
        "        Y_logits = model(encoded_text)\n",
        "        Y_probas = F.softmax(Y_logits[0, : , -1]/temperature, dim=-1)\n",
        "        predict_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n",
        "        return index_to_char[predict_char_id]"
      ],
      "metadata": {
        "id": "d9OTB7HvOO8E"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_char(model, text, n_chars=80, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(model, text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "fcFaVXCOi5Ny"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_char(model, \"To be or not to b\", temperature=0.01))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pT_zakOj6dl",
        "outputId": "382ed3c4-8390-497d-d9a9-53fbb14031b3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be so so should be so so should be so so should be so so should be so so should b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_char(model, \"To be or not to b\", temperature=0.4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6EXIUxRkGHA",
        "outputId": "fc362a5f-24c6-4c33-d26c-72bd086c85ba"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be the son of the noble lords, and will not will i may not shall be i would not k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_char(model, \"To be or not to b\", temperature=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZGOapZNkGLj",
        "outputId": "4095473b-cb7d-4896-dd77-c5a3a750164b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to bepvcvri,zgp&?:fkp:s,ixm.o;vtz ;rx'.&v'h'feehj3nne$!pdh?qjasq'&.y3t,lwb;hlozcqt. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Out.clear()  # clear Jupyter's `Out` variable which saves all the cell outputs\n",
        "del_vars([\"accuracy\", \"embed\", \"encoded\", \"encoded_text\", \"optimizer\", \"probs\",\n",
        "          \"samples\", \"x\", \"y\", \"shakespeare_text\", \"stateful_test_loader\",\n",
        "          \"stateful_train_loader\", \"Y_logits\", \"stateful_valid_loader\",\n",
        "          \"test_loader\", \"train_loader\", \"valid_loader\", \"xentropy\"])"
      ],
      "metadata": {
        "id": "SLWM-xNqU7eN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Loading the IMDB Dataset\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ewT_6MJ2kOtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oDgVztx7kGO0",
        "outputId": "89d1d34b-9031-4666-ec97-c0c88f49f2c8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "split = imdb_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
        "imdb_train_set, imdb_valid_set = split[\"train\"], split[\"test\"]\n",
        "imdb_test_set = imdb_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "FgyoRbpQkGRm"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_set[1][\"text\"], imdb_train_set[1][\"label\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I86niwZTkGUu",
        "outputId": "53f956fa-8c01-488d-de4f-cac2c3133f75"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"'The Rookie' was a wonderful movie about the second chances life holds for us and also puts an emotional thought over the audience, making them realize that your dreams can come true. If you loved 'Remember the Titans', 'The Rookie' is the movie for you!! It's the feel good movie of the year and it is the perfect movie for all ages. 'The Rookie' hits a major home run!\",\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_train_set[16][\"text\"], imdb_train_set[16][\"label\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBmaPtQykGYK",
        "outputId": "c2486674-7b62-4899-d933-ca0be531c737"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman, becomes a curious project to come out of gritty Warner Bros. Paul Lukas, reprising his Broadway role and winning the Best Actor Oscar, plays an anti-Nazi German underground leader fighting the Fascists, dragging his American wife and three children all over Europe before finding refuge in the States (via the Mexico border). They settle in Washington with the wife's wealthy mother and brother, though a boarder residing in the manor is immediately suspicious of the newcomers and spends an awful lot of time down at the German Embassy playing poker. It seems to take forever for this drama to find its focus, and when we realize what the heart of the material is (the wise, honest, direct refugees teaching the clueless, head-in-the-sand Americans how the world has suddenly changed), it seems a little patronizing--the viewer is quite literally put in the relatives' place, being lectured to. Lukas has several speeches in the third-act which undoubtedly won him the Academy Award, yet for the much of the picture he seems to do little but enter and exit, enter and exit. As his spouse, Bette Davis enunciates like nobody else and works her wide eyes to good advantage, but the role doesn't allow her much color. Their children (all with divergent accents!) are alternately humorous and annoying, and Geraldine Fitzgerald has a nothing role as a put-upon wife (and the disgruntled texture she brings to the part seems entirely wrong). The intent here was to tastefully, tactfully show us just because a (WWII-era) man may be German, that doesn't make him a Nazi sympathizer. We get that in the first few minutes; the rest of this tasteful, tactful movie is made up of exposition, defensive confrontation and, ultimately, compassion. It should be a heady mix, but instead it's rather dry-eyed and inert. ** from ****\",\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LRhwwZQJkGbS",
        "outputId": "bf753e47-7bf5-4b5d-fd17-6a1879c28ee4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers"
      ],
      "metadata": {
        "id": "hK_XWXbnaI_C"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_model = tokenizers.models.BPE(unk_none=\"<unk>\")\n",
        "bpe_tokenizer = tokenizers.Tokenizer(bpe_model)\n",
        "bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
        "\n",
        "special_token = [\"<pad>\", \"<unk>\"]\n",
        "bpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000, show_progress=True, special_tokens=special_token)\n",
        "train_reviews = [review[\"text\"].lower() for review in imdb_train_set]\n",
        "\n",
        "bpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer)"
      ],
      "metadata": {
        "id": "fnDLy1XPaJDU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_review = \"what an awesome movie! üòä\"\n",
        "\n",
        "bpe_encod = bpe_tokenizer.encode(some_review)\n",
        "bpe_encod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv9k46jkaJHf",
        "outputId": "8f99dbe6-b85e-4ca7-b7f7-1031b7ac3ba1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_encod.tokens, bpe_encod.ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv3vfBjMaJLv",
        "outputId": "b2fab161-c2ae-4dda-cd94-b705ca6e2699"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['what', 'an', 'aw', 'es', 'ome', 'movie', '!'],\n",
              " [303, 139, 373, 149, 240, 211, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.get_vocab()['what']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcalrbm1aJPl",
        "outputId": "7da234ec-a5e2-4ad4-990c-0e8baac543e2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.id_to_token(303)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L3kRPAGpaJTe",
        "outputId": "5a42346f-ad7b-4354-a7fd-54e30531535f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.decode([303, 139])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iBnbsdOTaJXY",
        "outputId": "bb92ce5d-4968-4181-c0b1-c8298e57bbd8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what an'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_encod.offsets # —Å–º–µ—â–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ —Å—Ç—Ä–æ–∫–µ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ2dfZtSaJb6",
        "outputId": "24f85eb4-61d5-4f6e-ec01-f7b10d31dadb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 4), (5, 7), (8, 10), (10, 12), (12, 15), (16, 21), (21, 22)]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.encode_batch(train_reviews[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbBjK5NBaJgR",
        "outputId": "72936dee-ee28-4e9e-9a4d-b40f3e712c50"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
              " Encoding(num_tokens=114, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
              " Encoding(num_tokens=285, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
        "bpe_tokenizer.enable_truncation(max_length=500)"
      ],
      "metadata": {
        "id": "M3ipt53haJkp"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_encodings = bpe_tokenizer.encode_batch(train_reviews[:3])\n",
        "bpe_batch_ids = torch.tensor([encoding.ids for encoding in bpe_encodings])\n",
        "bpe_batch_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o3kqMMzmaJpL",
        "outputId": "77f7060e-dea8-48af-a0af-e6db8d2abeb6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[159, 402, 176, 246,  61, 782, 156, 737, 252,  42, 239,  51, 154, 460,\n",
              "         917,  17, 272, 156, 737, 576, 215, 976, 275,  42, 199,  44, 554,  42,\n",
              "         192, 585,  57, 160, 259, 170, 157, 143, 138, 159, 402,  11, 589, 152,\n",
              "           5, 819, 168, 230,   5, 521, 924, 981, 962, 250,  61,  10,  60, 426,\n",
              "         526, 959,  60, 138, 199, 150, 319,  15, 363, 141, 957, 694,  47, 696,\n",
              "          61, 875, 138, 960, 337, 414, 140, 157, 385, 174, 433, 161, 221, 145,\n",
              "         213,  17, 549,  15, 151,  10,  60,  55, 416, 146, 407, 144, 182, 303,\n",
              "         151, 141,  17, 138, 547, 538, 528, 768,  54, 335,  42, 203,  44, 270,\n",
              "          46, 153, 876, 141, 919, 233, 522, 172, 141, 719, 162, 807, 279,  17,\n",
              "         138,  45,  66,  55, 188, 989, 156, 378, 698, 301, 296, 689, 212, 558,\n",
              "         926, 148,  17,  44, 270,  46, 141,  47, 279, 302, 171, 152, 787,  15,\n",
              "         153, 522, 172, 766, 205, 156, 234, 677, 161, 139, 513, 146, 370, 251,\n",
              "         219, 162, 197, 162, 166,  50, 265,  47, 266, 177,  50,  10, 172, 502,\n",
              "         499, 210,  42, 163,  63, 137,  10,  60, 387,  15, 209,  50, 183, 155,\n",
              "         177,  51, 186, 774, 143, 221, 145,  10,  60, 176, 246,  61, 301, 141,\n",
              "         460,  50, 136, 355,  17, 138, 778, 141, 137, 534, 271,  43, 160, 265,\n",
              "          63, 290, 179, 157,  15, 153, 959,  60, 206, 360, 266, 148,  17,   5,\n",
              "         222, 606, 241, 246,   5, 141, 139, 145, 154,  54, 287, 160, 885, 148,\n",
              "         199,  15, 153, 141, 142, 994, 157, 182, 236, 637, 221,  47, 489, 156,\n",
              "         159, 402, 153, 718, 219, 162, 197, 162, 166,  26,  17,  23, 215, 156,\n",
              "         586,   0,   0,   0,   0],\n",
              "        [ 10, 138, 198, 289, 175,  10, 192,  42, 725, 355, 211, 311, 138, 964,\n",
              "         161, 139, 513, 493, 204, 207,  60, 182, 244, 153, 434, 679,  60, 139,\n",
              "          46, 854, 904, 733, 394, 138, 870, 415,  15, 839, 419, 433, 544,  46,\n",
              "         177, 508,  45, 142, 188,  60, 313, 576, 986,  17, 249, 206, 180, 541,\n",
              "          10, 142, 984, 138, 899, 489,  10,  15,  10, 138, 198, 289, 175,  10,\n",
              "         141, 138, 211, 182, 206, 584, 151,  10,  60, 138, 580, 321, 211, 156,\n",
              "         138, 967, 153, 151, 141, 138, 257, 684, 211, 182, 221, 281, 149,  17,\n",
              "          10, 138, 198, 289, 175,  10,  49, 377,  42, 239,  51, 154,  49, 240,\n",
              "         841,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0],\n",
              "        [289,  15, 209, 398, 177, 442, 173,  42, 321, 211,  34, 371,  15, 226,\n",
              "         370,  15, 137, 306, 583, 137, 185,  17, 291, 809,  10,  61,  42, 833,\n",
              "         384, 146, 142, 994, 151,  17,  50, 920, 151, 268,  60, 413,  15, 712,\n",
              "         399, 160,  15, 137, 727,  17, 151,  10,  60, 434, 295,  45, 543, 344,\n",
              "         735, 417, 525, 153, 525,  17,  55, 357, 440, 205, 153, 495, 382, 253,\n",
              "         303, 295,  45, 543, 344,  15, 209, 226, 152, 417,  17,  50, 434,  47,\n",
              "         203,  61, 173, 211, 192, 719,  44, 188,  57,  66, 144, 650,  15, 363,\n",
              "          50, 653,  10,  61, 370, 439,  47, 377, 173, 761, 153, 138, 331,  17,\n",
              "          51, 553,  47, 460, 183,  66, 387,  60, 138, 783, 404, 652, 137, 173,\n",
              "         137, 159, 221, 323,  17, 183,  10,  60,  42, 222, 530, 759, 477,  15,\n",
              "         209,  50,  47, 203,  61, 183, 910, 234, 331, 450, 394, 138, 919,  17,\n",
              "          50, 360, 266, 177,  47, 151, 201, 138,  61, 205, 156, 138, 211,  15,\n",
              "         363, 343, 252, 428, 403, 249,  50, 369, 190, 930, 138, 211,  17, 259,\n",
              "         244,  15, 291, 382, 253, 735, 404, 205, 849, 315,  17, 155, 174, 207,\n",
              "         838,  60, 180,  56, 142, 617,  60, 137, 138, 899, 163, 806,  15, 209,\n",
              "         141, 518, 293, 520, 146, 455, 201, 137, 173, 211,  17, 138, 331, 308,\n",
              "         226, 370, 598, 290, 541,  15, 152,  50, 369, 204, 938,  17,  56,  49,\n",
              "         371,  17, 173, 141, 299, 306, 583, 137, 185,  17, 317, 307,  15, 182,\n",
              "         250,  15, 548, 173, 211, 141, 226, 202,  66, 685, 150,  15, 151, 141,\n",
              "         735, 404,  17, 306, 838, 286, 182,  45, 900, 243,  50,  50,  50,  29,\n",
              "          22,  17,  24,  18,  24]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = torch.tensor([encoding.attention_mask for encoding in bpe_encodings])\n",
        "attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-ik2R0xWaJt3",
        "outputId": "b95b5229-5683-4c5c-f1a3-c2f0f633a3d1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask.sum(dim=1), attention_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9z-MJY2hhqP",
        "outputId": "45519005-84bc-4019-aa32-7dd0d059db96"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([281, 114, 285]), torch.Size([3, 285]))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –î–ó –∑–∞–º–µ–Ω–∏ BpeTrainer –Ω–∞ WordPiceTrainer"
      ],
      "metadata": {
        "id": "xDy8bbgAjHm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "UpeDlPU_hht0"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True, max_length=500)"
      ],
      "metadata": {
        "id": "N_8h9O19hhxQ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n",
        "gpt2_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llD3XQ6yhh0t",
        "outputId": "686ba152-ad86-4d8a-d52f-f1e35694d528"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer.decode(gpt2_token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7dpP8YnFhh4I",
        "outputId": "d603c57a-b03f-4b96-9e53-0cfe8566f38f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stage adaptations often have a major fault. they often'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
        "                               truncation=True, max_length=500,\n",
        "                               return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "8BpCjNVvhh7R"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_encoding[\"input_ids\"][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-LiONgorhh-h",
        "outputId": "d014d154-8292-45ff-f442-d1cec2cd017a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027,\n",
              "          2411,  2272,  2041,  2559,  2066,  1037,  2143,  4950,  2001,  3432,\n",
              "          2872,  2006,  1996,  2754,  1006,  2107,  2004,  1000,  2305,  2388,\n",
              "          1000,  1007,  1012, 11430, 11320, 11368,  1005,  1055,  3257,  7906,\n",
              "          1996,  2143,  4142,  1010,  2029,  2003,  2926,  3697,  2144,  1996,\n",
              "          3861,  3253,  2032,  2053,  2613,  4119,  1012,  2145,  1010,  2009,\n",
              "          1005,  1055,  3835,  2000,  2298,  2012,  2005,  2054,  2009,  2003,\n",
              "          1012,  1996,  6370,  2090,  2745, 19881,  1998,  5696, 20726,  2003,\n",
              "          3243,  8235,  1012,  1996, 10949,  1997,  2037,  3276,  2024, 11341,\n",
              "          1012, 19881,  2003, 10392,  2004,  2467,  1010,  1998, 20726,  4152,\n",
              "          2028,  1997,  2010,  2261,  9592,  2000,  2428,  2552,  1012,  1026,\n",
              "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1045, 18766,  2008,\n",
              "          1045,  1005,  2310,  2196,  2464, 11209, 20206,  1005,  1055,  2377,\n",
              "          1010,  2021,  1045,  2963,  2008,  6108,  2811,  2239,  5297,  1005,\n",
              "          1055,  6789,  2003, 11633,  1012,  1996,  5896,  2003, 11757,  9530,\n",
              "          6767,  7630,  3064,  1010,  1998,  7906,  2017, 16986,  1012,  1000,\n",
              "          2331,  6494,  2361,  1000,  2003,  2019,  8216,  2135, 14036,  2143,\n",
              "          1010,  1998,  2003,  6749,  2005,  3053,  2035,  4599,  1997,  2754,\n",
              "          1998,  3898,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
              "          1028,  1021,  1012,  1018,  2041,  1997,  2184,   102,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0],\n",
              "        [  101,  1005,  1996,  8305,  1005,  2001,  1037,  6919,  3185,  2055,\n",
              "          1996,  2117,  9592,  2166,  4324,  2005,  2149,  1998,  2036,  8509,\n",
              "          2019,  6832,  2245,  2058,  1996,  4378,  1010,  2437,  2068,  5382,\n",
              "          2008,  2115,  5544,  2064,  2272,  2995,  1012,  2065,  2017,  3866,\n",
              "          1005,  3342,  1996, 13785,  1005,  1010,  1005,  1996,  8305,  1005,\n",
              "          2003,  1996,  3185,  2005,  2017,   999,   999,  2009,  1005,  1055,\n",
              "          1996,  2514,  2204,  3185,  1997,  1996,  2095,  1998,  2009,  2003,\n",
              "          1996,  3819,  3185,  2005,  2035,  5535,  1012,  1005,  1996,  8305,\n",
              "          1005,  4978,  1037,  2350,  2188,  2448,   999,   102,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0],\n",
              "        [  101,  7929,  1010,  2021,  2515,  2008,  2191,  2023,  1037,  2204,\n",
              "          3185,  1029,  2092,  1010,  2025,  2428,  1010,  1999,  2026,  5448,\n",
              "          1012,  2045,  3475,  1005,  1056,  1037,  2878,  2843,  2000, 16755,\n",
              "          2009,  1012,  1045,  2179,  2009,  2200,  4030,  1010,  6945, 19426,\n",
              "          1010,  1999,  2755,  1012,  2009,  1005,  1055,  2036, 21425,  3492,\n",
              "          2172,  2083,  1998,  2083,  1012,  2193,  2028,  1998,  2048,  2020,\n",
              "          5399, 21425,  1010,  2021,  2025,  2004,  2172,  1012,  1045,  2036,\n",
              "          2371,  2023,  3185,  2001,  3243,  3409,  2100,  2012,  2335,  1010,\n",
              "          2029,  1045,  2134,  1005,  1056,  2428,  2228, 16142,  2023,  2186,\n",
              "          1998,  1996,  2839,  1012,  5076,  6904, 14844,  3248,  1996,  2364,\n",
              "          2919,  3124,  1999,  2023, 18932,  1012,  2002,  1005,  1055,  1037,\n",
              "         11519,  2438,  3364,  1010,  2021,  1045,  2371,  2002,  2209,  2010,\n",
              "          2839,  2205,  2058,  1996,  2327,  1012,  1045,  3984,  2008,  4906,\n",
              "          2007,  1996,  4309,  1997,  1996,  3185,  1010,  2029,  2052,  2031,\n",
              "          2042,  2307,  2065,  1045,  2018,  4669,  1996,  3185,  1012,  4606,\n",
              "          1010,  2045,  2020,  2070,  3492,  2919,  2028, 11197,  2015,  1012,\n",
              "          7779, 29536, 14540,  9541,  5651,  1999,  1996,  2516,  2535,  1010,\n",
              "          2021,  2003,  2445,  2210,  2000,  2147,  2007,  1999,  2023,  3185,\n",
              "          1012,  1996,  2839,  2038,  2025,  2428,  7964,  1010,  2004,  1045,\n",
              "          2018,  5113,  1012,  2821,  2092,  1012,  2023,  2003,  2074,  2026,\n",
              "          5448,  1012,  4312,  1010,  2005,  2033,  1010,  2096,  2023,  3185,\n",
              "          2003,  2025, 11113,  7274,  9067,  1010,  2009,  2003,  3492,  2919,\n",
              "          1012,  2026,  3789,  2005,  2601,  2386,  3523,  1024,  1017,  1012,\n",
              "          1019,  1013,  1019,   102]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_encoding[\"attention_mask\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ufNnH7urhiBz",
        "outputId": "ae4cb706-7c51-42a2-8039-1a9b481c0d03"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=bpe_tokenizer)\n",
        "hf_encodings = hf_tokenizer(train_reviews[:3], padding=True, truncation=True, max_length=500, return_tensors=\"pt\")\n",
        "hf_encodings[\"input_ids\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ayViFZ6thiFj",
        "outputId": "20471f86-6c83-4bf3-fd33-18d42c48efe9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[159, 402, 176, 246,  61, 782, 156, 737, 252,  42, 239,  51, 154, 460,\n",
              "         917,  17, 272, 156, 737, 576, 215, 976, 275,  42, 199,  44, 554,  42,\n",
              "         192, 585,  57, 160, 259, 170, 157, 143, 138, 159, 402,  11, 589, 152,\n",
              "           5, 819, 168, 230,   5, 521, 924, 981, 962, 250,  61,  10,  60, 426,\n",
              "         526, 959,  60, 138, 199, 150, 319,  15, 363, 141, 957, 694,  47, 696,\n",
              "          61, 875, 138, 960, 337, 414, 140, 157, 385, 174, 433, 161, 221, 145,\n",
              "         213,  17, 549,  15, 151,  10,  60,  55, 416, 146, 407, 144, 182, 303,\n",
              "         151, 141,  17, 138, 547, 538, 528, 768,  54, 335,  42, 203,  44, 270,\n",
              "          46, 153, 876, 141, 919, 233, 522, 172, 141, 719, 162, 807, 279,  17,\n",
              "         138,  45,  66,  55, 188, 989, 156, 378, 698, 301, 296, 689, 212, 558,\n",
              "         926, 148,  17,  44, 270,  46, 141,  47, 279, 302, 171, 152, 787,  15,\n",
              "         153, 522, 172, 766, 205, 156, 234, 677, 161, 139, 513, 146, 370, 251,\n",
              "         219, 162, 197, 162, 166,  50, 265,  47, 266, 177,  50,  10, 172, 502,\n",
              "         499, 210,  42, 163,  63, 137,  10,  60, 387,  15, 209,  50, 183, 155,\n",
              "         177,  51, 186, 774, 143, 221, 145,  10,  60, 176, 246,  61, 301, 141,\n",
              "         460,  50, 136, 355,  17, 138, 778, 141, 137, 534, 271,  43, 160, 265,\n",
              "          63, 290, 179, 157,  15, 153, 959,  60, 206, 360, 266, 148,  17,   5,\n",
              "         222, 606, 241, 246,   5, 141, 139, 145, 154,  54, 287, 160, 885, 148,\n",
              "         199,  15, 153, 141, 142, 994, 157, 182, 236, 637, 221,  47, 489, 156,\n",
              "         159, 402, 153, 718, 219, 162, 197, 162, 166,  26,  17,  23, 215, 156,\n",
              "         586,   0,   0,   0,   0],\n",
              "        [ 10, 138, 198, 289, 175,  10, 192,  42, 725, 355, 211, 311, 138, 964,\n",
              "         161, 139, 513, 493, 204, 207,  60, 182, 244, 153, 434, 679,  60, 139,\n",
              "          46, 854, 904, 733, 394, 138, 870, 415,  15, 839, 419, 433, 544,  46,\n",
              "         177, 508,  45, 142, 188,  60, 313, 576, 986,  17, 249, 206, 180, 541,\n",
              "          10, 142, 984, 138, 899, 489,  10,  15,  10, 138, 198, 289, 175,  10,\n",
              "         141, 138, 211, 182, 206, 584, 151,  10,  60, 138, 580, 321, 211, 156,\n",
              "         138, 967, 153, 151, 141, 138, 257, 684, 211, 182, 221, 281, 149,  17,\n",
              "          10, 138, 198, 289, 175,  10,  49, 377,  42, 239,  51, 154,  49, 240,\n",
              "         841,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0],\n",
              "        [289,  15, 209, 398, 177, 442, 173,  42, 321, 211,  34, 371,  15, 226,\n",
              "         370,  15, 137, 306, 583, 137, 185,  17, 291, 809,  10,  61,  42, 833,\n",
              "         384, 146, 142, 994, 151,  17,  50, 920, 151, 268,  60, 413,  15, 712,\n",
              "         399, 160,  15, 137, 727,  17, 151,  10,  60, 434, 295,  45, 543, 344,\n",
              "         735, 417, 525, 153, 525,  17,  55, 357, 440, 205, 153, 495, 382, 253,\n",
              "         303, 295,  45, 543, 344,  15, 209, 226, 152, 417,  17,  50, 434,  47,\n",
              "         203,  61, 173, 211, 192, 719,  44, 188,  57,  66, 144, 650,  15, 363,\n",
              "          50, 653,  10,  61, 370, 439,  47, 377, 173, 761, 153, 138, 331,  17,\n",
              "          51, 553,  47, 460, 183,  66, 387,  60, 138, 783, 404, 652, 137, 173,\n",
              "         137, 159, 221, 323,  17, 183,  10,  60,  42, 222, 530, 759, 477,  15,\n",
              "         209,  50,  47, 203,  61, 183, 910, 234, 331, 450, 394, 138, 919,  17,\n",
              "          50, 360, 266, 177,  47, 151, 201, 138,  61, 205, 156, 138, 211,  15,\n",
              "         363, 343, 252, 428, 403, 249,  50, 369, 190, 930, 138, 211,  17, 259,\n",
              "         244,  15, 291, 382, 253, 735, 404, 205, 849, 315,  17, 155, 174, 207,\n",
              "         838,  60, 180,  56, 142, 617,  60, 137, 138, 899, 163, 806,  15, 209,\n",
              "         141, 518, 293, 520, 146, 455, 201, 137, 173, 211,  17, 138, 331, 308,\n",
              "         226, 370, 598, 290, 541,  15, 152,  50, 369, 204, 938,  17,  56,  49,\n",
              "         371,  17, 173, 141, 299, 306, 583, 137, 185,  17, 317, 307,  15, 182,\n",
              "         250,  15, 548, 173, 211, 141, 226, 202,  66, 685, 150,  15, 151, 141,\n",
              "         735, 404,  17, 306, 838, 286, 182,  45, 900, 243,  50,  50,  50,  29,\n",
              "          22,  17,  24,  18,  24]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building and Training a Sentiment Analysis Model**"
      ],
      "metadata": {
        "id": "g9z3MhBhxu83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, tokenizer=bert_tokenizator):\n",
        "    reviews = [review[\"text\"] for review in batch]\n",
        "    labels = [review[\"label\"] for review in batch]\n",
        "    encodings = tokenizer(reviews, padding=True, truncation=True, max_length=200, return_tensors='pt')\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return encodings, labels\n"
      ],
      "metadata": {
        "id": "k3nJk09UhiI-"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, tokenizer=bert_tokenizator):\n",
        "    reviews = [review[\"text\"] for review in batch]\n",
        "    labels = [[review[\"label\"]] for review in batch]\n",
        "    encodings = tokenizer(reviews, padding=True, truncation=True,\n",
        "                          max_length=200, return_tensors=\"pt\")\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return encodings, labels\n",
        "\n",
        "batch_size = 256\n",
        "imdb_train_loader = DataLoader(imdb_train_set, batch_size=batch_size,\n",
        "                               collate_fn=collate_fn, shuffle=True)\n",
        "imdb_valid_loader = DataLoader(imdb_valid_set, batch_size=batch_size,\n",
        "                               collate_fn=collate_fn)\n",
        "imdb_test_loader = DataLoader(imdb_test_set, batch_size=batch_size,\n",
        "                              collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "abR-0jbthiMc"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysisModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layers=2, embed_dim=128, hidden_dim=64,\n",
        "                 pad_id=0, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim,\n",
        "                                  padding_idx=pad_id)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encodings):\n",
        "        embeddings = self.embed(encodings[\"input_ids\"])\n",
        "        _outputs, hidden_states = self.gru(embeddings)\n",
        "        return self.output(hidden_states[-1])"
      ],
      "metadata": {
        "id": "OULKSq2shiPr"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "sequence = torch.tensor([[1,2,0,0], [5,6,7,8]])\n",
        "packed = pack_padded_sequence(sequence, lengths=(2,4), enforce_sorted=False, batch_first=True)\n",
        "packed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEZvf8OPhiS_",
        "outputId": "dea06ef5-c7a2-45cb-d447-5e1e72cf9754"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([5, 1, 6, 2, 7, 8]), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded, lengths = pad_packed_sequence(packed, batch_first=True)\n",
        "padded, lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYDsnlMJhiWt",
        "outputId": "cdd826c6-a191-4e22-8fdd-9fe3e430b739"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 2, 0, 0],\n",
              "         [5, 6, 7, 8]]),\n",
              " tensor([2, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysisModelPackedSeq(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layers=2, embed_dim=128,\n",
        "                 hidden_dim=64, pad_id=0, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim,\n",
        "                                  padding_idx=pad_id)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encodings):\n",
        "        embeddings = self.embed(encodings[\"input_ids\"])\n",
        "        lengths = encodings[\"attention_mask\"].sum(dim=1)                      # <= line added\n",
        "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),      # <= line added\n",
        "                                      batch_first=True, enforce_sorted=False) # <= line added\n",
        "        _outputs, hidden_states = self.gru(packed)                            # <= line changed\n",
        "        return self.output(hidden_states[-1])"
      ],
      "metadata": {
        "id": "xCCMQP0bhiac"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "vocab_size = bert_tokenizer.vocab_size\n",
        "imdb_model_ps = SentimentAnalysisModelPackedSeq(vocab_size).to(device)\n",
        "\n",
        "n_epochs = 1\n",
        "xentropy = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.NAdam(imdb_model_ps.parameters())\n",
        "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
        "\n",
        "history = train(imdb_model_ps, optimizer, xentropy, accuracy,\n",
        "                imdb_train_loader, imdb_valid_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ0X2IkahieW",
        "outputId": "032860b5-89fb-4d9e-d178-cf8f43b1a69f"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1,train loss: 0.6726,train metrics: 58.27%, valid metrics: 59.66% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bidirectional RNNs**"
      ],
      "metadata": {
        "id": "YUM-xHHoKTVB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQFfoB-mhih7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tn_9wVYKhilr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tVShsr4hipR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jO2H3wqRhis4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6ZORNqVhiwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fztouy73hi0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}